* HDFS

Optimised for large files, not really many small files

Stores data in blocks, and these blocks are stored across multiple servers. Data is stored in more than one block for fail-over

Single Name Node - keeps track of where all blocks live. Knows the file name and path. Contains an edit block so it knows what has been created and edited.
Data Nodes - contains data

** Reading a file

Client node talks to Name node to see which blocks to go look at, then client node goes to the actual data nodes.

** Writing a file

Client node tells name node the file node.
Client talks to single data node to write file, and data nodes will talk to each other to store the data in replicated manner
Data nodes send back a acknowledged and client node tells name node where it was stored and that it has been written

** Name node DR

Always only have 1 active Name node, several options to avoid losing data:

- backup. In case of name node failure, bring up new name node with backups
- Secondary namenode. Maintains merged copy of edit log you can restore from.
- HDFS Federation. Each namenode manages a specific namespace volume. Separate name nodes for separate volumes (Namenode can reach name list limit)
- HDFS High Available. If you can't afford downtime. Uses a shared edit log. Zookeeper tracks active namenode and extreme measures to ensure only one namenode is used at a time

** Using HDFS

- UI (Ambari) - looks like a giant file system
- CLI
- HTTP directly or via HDFS Proxies
- Java interface
- NFS Gateway (network file system), way of remoting a remote file system on the server. Will just look like another mount point on your linux box.

* MapReduce
2 stages: Map and Reduce.

Map phase includes shuffling and sorting. 

Example using python. Can run locally with python script.py u.data 
or on hadoop cluster with python scipt.py -r hadoop --hadoop-streaming-jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar u.data

#+BEGIN_SRC python
  from mrjob.job import MRJob
  from mrjob.step import MRStep

  class RatingsBreakdown(MRJob):
      def steps(self):
          # Takes in 2 parameters - a mapper and a reducer
          # in this case the mapper is the output of a mapper and reducer
          return [
              MRStep(mapper=self.mapper_get_ratings,
                     reducer=self.reducer_count_ratings),
              MRStep(reducer=self,reducer_sorted_output)
              ]

      def mapper_get_ratings(self, _, line):
          (userID, movieID, rating, timestamp) = line.split('\t')
          yield movieID, 1

      def reducer_count_ratings(self, key, values):
          yield str(sum(values)).zfill(5), key #zfill filss the number up to 5 spaces e.g. 00004

      def reducer_sorted_output(self, count, movies):
          for movie in movies:
              yield movie, count


  if __name__ == '__main__':
      RatingsBreakdown.run()
#+END_SRC

* Pig

Built on top of MapReduce or Tez, a SQL-like interface (Pig Latin) to write MapReduce pipelines without having to use the complex MapReduce

Can run this using grunt, script or ambarai / hue

Example on MapReduce: load movie data, transform to show average ratings, filter to show only ratings > 4, join datasets to get names of movies and not ID's.
To execute on Tez (faster): check the "execute on Tez" checkbox and execute.
(Tez uses acyclic graphs for computation)

Each of the expressions is called a 'relation'

#+BEGIN_SRC text
  ratings = LOAD '/user/maria_dev/ml-100k/u.data' AS (userId:int, movieId:int, rating:int, ratingTime:int);

  metadata = LOAD '/user/maria_dev/ml-100k/u.item' USING PigStorage('|')
      AS (movieID:int, movieTitle:chararray, releaseDate:chararray, videoRelease:chararray, imdbLink:chararray);
    
  nameLookup = FOREACH metadata GENERATE movieID, movieTitle,
      ToUnixTime(ToDate(releaseDate, 'dd-MMM-yyyy')) AS releaseTime;
    
  ratingsByMovie = GROUP ratings BY movieId;

  avgRatings = FOREACH ratingsByMovie GENERATE group AS movieID, AVG(ratings.rating) AS avgRating;

  fiveStarMovies = FILTER avgRatings BY avgRating > 4.0;

  fiveStarsWithData = JOIN fiveStarMovies BY  movieID, nameLookup BY movieID;

  oldestFiveStarMovies = ORDER fiveStarsWithData BY nameLookup::releaseTime;

  DUMP oldestFiveStarMovies;
#+END_SRC

** Basic commands
  - LOAD STORE DUMP
  - STORE ratings INTO 'outRatings' USING PigStorage(':');
  - FILTER DISTINCT FOREACH/GENERATE MAPREDUCE STREAM STREAM SAMPLE
  - JOIN COGROUP GROUP CUBE
  - ORDER RANK LIMIT
  - UNION SPLIT

** Diagnostics
  - DESCRIBE
  - EXPLAIN
  - ILLUSTRATE
 
** UDF's
  - REGISTER
  - DEFINE
  - IMPORT

** Some other functions and loaders
  - AVG CONCAT COUNT MAX MIN SIZE SUM
  - PigStorage
  - TextLoader
  - JsonLoader
  - AvroStorage
  - ParquetLoader
  - OrcStorage
  - HBaseStorage

* Spark 

DAG Engine (directed acyclic graph) optimizes workflows

Components of spark part of spark core

  - Spark streaming
  - Spark SQL
  - MLLib
  - GraphX

ssh into hadoop cluster, run: spark-submit file.py

Can start a thrift service with spark sql and connect to it and query it

More details in spark course

* Hive

On top of Mapreduce and Tez. 

Short for HiveQL - allows you to query HDFS data using SQL syntax

Basically smokes and mirrors to make it seem like you're working with a relational database

** Why not hive?

 - high latency - not appropriate for OLTP
 - stores data de-normalized
 - SQL is limited in what it can do (Pig, spark allows more complex stuff)
 - No transactions
 - no record-level updates, inserts, deletes

In hive view, can upload table and then write queries in HQL.

Can create views as well (which gets persisted as with usual relational db)

** How does hive work?

Schema on read

Hive takes unstructured data and applies a schema to it as it reads, where relational databases write the schema first and read data according to that schema (schema on write)

 LOAD DATA - hive will move data from a distributed filesystem into Hive (the raw data)
 LOAD DATA LOCAL - copies data from local filesystem into Hive
 Managed vs External tables: managed tables are where hive takes control of that data. 
To create external table use "CREATE EXTERNAL TABLE", give it a location and then hive doesn't take ownership of it. Thus, dropping data will drop metadata but not the actual data.

*** Partitioning

You can store your data in partitioned subdirectories (optimisation)

E.g. 
#+BEGIN_SRC sql
 CREATE TABLE person(
   name STRING,
   address STRUCT<street: String, city: String>
 )
PARTITIONED BY (country STRING)
#+END_SRC

Can use it through Ambari / Hue; JDBC/ODBC server; Thrift service (but remember hive is not suitable for OLTP); via Oozie



* Scqoop

Meant for large datasets.

Kicks of mapreduce jobs to handle importing and exporting your data

Takes data from mysql/postgres etc, distributes processing across several parallel mappers and writing to HDFS.

Command line tool:

#+BEGIN_SRC bash
  sqoop import --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies
#+END_SRC

To add to hive instead add '--hive-import'
To set the number of mappers add -m 1

Can do incremental imports in sqoop (can be used to keep table up to date) by using --check-column (to check like a date column) and --last-value 

To export from hive table to mysql: (mysql table needs to exist)

#+BEGIN_SRC bash
  sqoop export --connect jdbc:mysql://localhost/movielens -m 1 --driver com.mysql.jdbc.Driver --table exported_movies --export-dir /apps/hive/warehouse/movies --input-fields-terminated-by '\0001'
#+END_SRC

* Integrating hadoop with nosql

