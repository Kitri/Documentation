#+TITLE: Apache Spark with Scala - Hands On
#+OPTIONS: toc:nil num:0 H:4 ^:nil pri:t html-style:nil
#+HTML_HEAD:  <link id="pagestyle" rel="stylesheet" type="text/css" href="../org.css"/>
#+HTML_LINK_HOME: ../index.html
#+TOC: headlines 1

* Course details
Apache Spark with Sclaa - Hands on with Big Data!

Git repository: https://bitbucket.org/mercia_malan/sparkplay/src/master/

* What's new in Spark 3?

 - Deprecating MLLIB using RDDs (Dataframes one still available)
 - Deprectate python2
 - GraphX => SparkGraph (based on cypherquery language)

 - Better performance
 - Can use GPU hardware now, so you can use 3rd party deep learning add-ons
 - Better kubernetes support
 - Support binary files to be loaded into a dataframe
 - ACID support and data lake 

* RDD - Resilient Distributed Dataset

Lazy evaluation

#+BEGIN_SRC scala
  val numbs = parallelize(List(1,2,3,4))
  sc.textFile("file:/// ....") //or s3 or hdfs
  rows = hiveCtx.sql("SELECT name...")

#+END_SRC

Can read from JDBC, Cassandra, HBase, Elasticsearch, JSON, CSV...

** Operations on RDDs

map, flatmap, filter, distinct, sample, union, intersection, subtract, cartesian

collect, count, countByValue, take, top, reduce ...


*** map and flatmap

#+BEGIN_SRC scala
  rdd.map(x => x * x)
  rdd.flatMap(x => x.split(" ")) //Taking a sentence and splitting it into a list of words
  rdd.flatMap(x => x.split("\\W+")) //using regex - split on "word"
#+END_SRC

** PairRDDS

In scala, just convert to tuples then you have key,value RDDs

Can then use some pair functions like reduceByKey, groupByKey, sortByKey, keys(), values(), mapValues(), flatMapValues()
join, rightOuterJoin, leftOuterJoin, cogroup, subtractByKey (Though will use sparksql or dataframes in modern spark)


#+BEGIN_SRC scala
  rdd.reduceByKey((x,y) => x + y)

#+END_SRC

* SparkSQL, DataFrames and DataSets

** DataFrame

 - Extends from RDD
 - Contains row objects
 - Has a schema
 - Can run actual sql queries on it
 - Easy to communicate to relational db's
 - Schema inferred at runtime

** DataSet

Works best with structured data!

documentation: spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html


 - A dataframe is really just a dataset of Row objects [DataSet[Row]]
 - Dataset knows schema at compile time
 - Datasets can only be used with Java and Scala (Python is not a compiled language)
 - RDD .toDS()
 - Can be more efficient, can be serialized very efficiently

*** SparkSession

Need a sparksession. Can get sparkcontext from sparksession

Note: Have to stop a session

To create a spark session:

#+BEGIN_SRC scala
  val spark = SparkSession
    .builder
    .appName("FriendsByAgeDS")
    .master("local[*]")
    .getOrCreate()

  //remember to stop at the end
  spark.stop()
#+END_SRC


*** Creating a dataset

first create a case class with the schema, then read in the data with "as[caseclass]"

If your data has headers, you can use option header true +
inferschema. If it doesn't, inferschema might fail and thus you need
to create an explicit schema and pass it in.

Extra part: add logging level as error

#+BEGIN_SRC scala
  import org.apache.spark.sql.SparkSession
  import org.apache.log4j._

  case class Person(id:Int, name:String, age:Int, friends:Long)

  //main function
  Logger.getLogger("org").setLevel(Level.ERROR)

  val ds = spark.read
    .option("header", "true")
    .option("inferSchema", "true")
    .csv("src/main/resources/fakefriends.csv")
    .as[Person]
    .cache()

  //Or use implicits to convert to dataset from dataframe

  import spark.implicits._
  val ds = spark.read.text("fakefriends.csv").as[Person]
#+END_SRC


Can also specify the schema as we're readying:

#+BEGIN_SRC scala
  import org.apache.spark.sql.types._

  val personSchema = new StructType()
        .add("id", StringType, nullable = true)
        .add("name", StringType, nullable = true)
        .add("age", IntegerType, nullable = true)
        .add("friends", LongType, nullable = true)

  val ds = spark.read
    .schema(personSchema)
    .csv("xx.csv")
    .as[Person]
#+END_SRC


*** Examples

#+BEGIN_SRC scala
  //show shows the first 20 rows by default. Pass in an integer to set how many rows to show
  ds.show()
  ds.select("someField")
  ds.filter(ds("someField")>200)
  ds.groupBy(ds("someField)).mean()
  ds.rdd().map(mapperFunction)
#+END_SRC

More advanced:

#+BEGIN_SRC scala
  import org.apache.spark.sql.functions._
  ds.groupBy("age").agg(round(avg("friends"), 2).alias("friends_avg")).sort("age").show()

  //note syntax, use $ to pass column names into sql functions, and weird =!= for not equal to
  ds.select(explode(split($"value", "\\W+")).alias("word")).filter($"word" =!= "")
#+END_SRC

Look at degreesOfSeperation.scala for example of using an accumulator to do a BFS 

*** Useful functions

  - withColumn (see git code MinTemperaturesDS) to add a new column


** SparkSQL

Exposes a jdbc / odbc server so you can use it just like a sql db

** UDF

spark.sql.funcitons.udf - can create user defined functions

* Running spark on a cluster

Before running:
 - make sure all the paths in the project don't point to your local drive
 - create jar file
 - use spark-submit <mainClass> --jars <dependencies> --files <filesToUse> <pathToJar>

** spark-submit parameters

--master [yarn/hostname:port/mesos://masternode:port] //note master in your SparkConf will override this!
--num-executors [num] //default: 2; have to explicitly set this with YARN
--executor-memory //make sure this does not exceed physical memory
--total-executor-cores


** Set up local spark 

 - go to spark website, download zip file (using 3.0.0 with hadoop 2.7 in this course)
 - decompress zip file to somewhere you can find it again
 - create jar from project

*** Using SBT

Project should have the following subdirectory structure:

src - main - scala
project

Create assembly.sbt file inside the project folder with the following line:
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10") 

Check latest sbt-assembly documentation as this will change over time

Create build.sbt:
(Spark 3 requires scala 2.12 - check scala version to spark version)

#+BEGIN_SRC sbt
name := "PopularMovies"

version := "1.0"

organization := "com.sundogsoftware"

scalaVersion := "2.12.3"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "3.0.0" % "provided",
  "org.apache.spark" %% "spark-sql" % "3.0.0" % "providec"
)
#+END_SRC

provided: We assume the package will be pre-installed where we run it on, thus the package will not be included in the compiled jar file

Run: sbt assembly

** Amazon elastic MapReduce (EMR)

Quick way to create a cluster with spark, hadoop and yarn
(You pay by the hour-instance and for network and storage IO)

Remember to terminate your cluster when you're done! Else you'll get charged quite a lot....

Do dev and testing locally with a subset of data, and then only publish it to the cloud with your whole dataset.

EMR should have the right defaults already (memory and that stuff)

*** Setup

 - Create AWS account
 - Create an EC2 key pair and download the .pem file
 - Need a terminal

Upload files and jar to an S3 bucket (He uses S3 Browser)

- Go to EMR or search "Elastic Map Reduce"
- Create new cluster; launch mode - cluster; applications: Spark + hadoop + yarn
  - hardware config m5xlarge; 3 (1 master and 2 core)

Master and core should be in running state.

Check Master public DNS, click on "ssh" it will show you exactly how to connect (hadoop@server.amazonaws.com)

Might need to go to security group for master node and click on inbound to create a rule for ssh on port 22 for your machine IP

1. SSH into EMR
2. Copy data from s3: aws cp s3://directory ./
3. spark-submit jarfile.jar
4. see results :D 
5. Terminate cluster: Go to EMR dashboard and click "Terminate"

* Partitioning

Splits data across executors. Spark does this automatically, but sometimes needs a hint to optimise.

Use .repartition() on a Dataframe, or .partitionBy() on an RDD before running a large operation that benefits from partitioning
  - join(), cogroup(), groupWith(), lookup(), combineByKey() .. any join, group or reduceby operations

In general:
 - At least as many partitions as executors to take full advantage of your cluster
 - Too many partitions results in too much shuffling
 - At least as many partitions as you have cores, or executors that fit within your available memory
 - 100 is usually a reasonable place to start for large operations

* Tweaking settings

Generally, do not set this in driver or commandline, unless really needed. Most pre-set clusters already set this effectively

Can manually set memory per executor or master, but both these will be predefined on your cluster

spark-submit --executor-memory 1g
             --master yarn
* Spark ML

Can achieve the same stuff with python, but if you have a really large dataset, spark ml might be useful

** What can it do?
 - Feature extraction
   TIDF - Term Frequency / Inverse Document Frequency useful to search
 - Basic Statistics
   Chi-squared test, pearson spearman correlation, min, max, mean, variance
 - linear regression, logistic regression
 - support vector machines
 - naive bayes classifier
 - decision trees
 - k-means clustering (unsupervised learning)
 - principle component analysis (PCA), singular value decomposition (SVD) - dimensionality reduction techniques
 - recommendations using Alternating Least Squares 
