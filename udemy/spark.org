* Course details
Apache Spark with Sclaa - Hands on with Big Data!

Git repository: https://bitbucket.org/mercia_malan/sparkplay/src/master/

* What's new in Spark 3?

 - Deprecating MLLIB using RDDs (Dataframes one still available)
 - Deprectate python2
 - GraphX => SparkGraph (based on cypherquery language)

 - Better performance
 - Can use GPU hardware now, so you can use 3rd party deep learning add-ons
 - Better kubernetes support
 - Support binary files to be loaded into a dataframe
 - ACID support and data lake 

* RDD - Resilient Distributed Dataset

Lazy evaluation

#+BEGIN_SRC scala
  val numbs = parallelize(List(1,2,3,4))
  sc.textFile("file:/// ....") //or s3 or hdfs
  rows = hiveCtx.sql("SELECT name...")

#+END_SRC

Can read from JDBC, Cassandra, HBase, Elasticsearch, JSON, CSV...

** Operations on RDDs

map, flatmap, filter, distinct, sample, union, intersection, subtract, cartesian

collect, count, countByValue, take, top, reduce ...


*** map and flatmap

#+BEGIN_SRC scala
  rdd.map(x => x * x)
  rdd.flatMap(x => x.split(" ")) //Taking a sentence and splitting it into a list of words
  rdd.flatMap(x => x.split("\\W+")) //using regex - split on "word"
#+END_SRC

** PairRDDS

In scala, just convert to tuples then you have key,value RDDs

Can then use some pair functions like reduceByKey, groupByKey, sortByKey, keys(), values(), mapValues(), flatMapValues()
join, rightOuterJoin, leftOuterJoin, cogroup, subtractByKey (Though will use sparksql or dataframes in modern spark)


#+BEGIN_SRC scala
  rdd.reduceByKey((x,y) => x + y)

#+END_SRC


* SparkSQL, DataFrames and DataSets

** DataFrame

 - Extends from RDD
 - Contains row objects
 - Has a schema
 - Can run actual sql queries on it
 - Easy to communicate to relational db's
 - Schema inferred at runtime

** DataSet

 - A dataframe is really just a dataset of Row objects [DataSet[Row]]
 - Dataset knows schema at compile time
 - Datasets can only be used with Java and Scala (Python is not a compiled language)
 - RDD .toDS()
 - Can be more efficient, can be serialized very efficiently

*** SparkSession

Need a sparksession. Can get sparkcontext from sparksession

Note: Have to stop a session

*** examples

#+BEGIN_SRC scala
  ds.show()
  ds.select("someField")
  ds.filter(ds("someField")>200)
  ds.groupBy(ds("someField)).mean()
  ds.rdd().map(mapperFunction)
#+END_SRC

** SparkSQL

Exposes a jdbc / odbc server so you can use it just like a sql db

*** UDF

spark.sql.funcitons.udf - can create user defined functions
