* Course details
Apache Spark with Sclaa - Hands on with Big Data!

Git repository: https://bitbucket.org/mercia_malan/sparkplay/src/master/

* What's new in Spark 3?

 - Deprecating MLLIB using RDDs (Dataframes one still available)
 - Deprectate python2
 - GraphX => SparkGraph (based on cypherquery language)

 - Better performance
 - Can use GPU hardware now, so you can use 3rd party deep learning add-ons
 - Better kubernetes support
 - Support binary files to be loaded into a dataframe
 - ACID support and data lake 

* RDD - Resilient Distributed Dataset

Lazy evaluation

#+BEGIN_SRC scala
  val numbs = parallelize(List(1,2,3,4))
  sc.textFile("file:/// ....") //or s3 or hdfs
  rows = hiveCtx.sql("SELECT name...")

#+END_SRC

Can read from JDBC, Cassandra, HBase, Elasticsearch, JSON, CSV...

** Operations on RDDs

map, flatmap, filter, distinct, sample, union, intersection, subtract, cartesian

collect, count, countByValue, take, top, reduce ...


*** map and flatmap

#+BEGIN_SRC scala
  rdd.map(x => x * x)
  rdd.flatMap(x => x.split(" ")) //Taking a sentence and splitting it into a list of words
  rdd.flatMap(x => x.split("\\W+")) //using regex - split on "word"
#+END_SRC

** PairRDDS

In scala, just convert to tuples then you have key,value RDDs

Can then use some pair functions like reduceByKey, groupByKey, sortByKey, keys(), values(), mapValues(), flatMapValues()
join, rightOuterJoin, leftOuterJoin, cogroup, subtractByKey (Though will use sparksql or dataframes in modern spark)


#+BEGIN_SRC scala
  rdd.reduceByKey((x,y) => x + y)

#+END_SRC

* SparkSQL, DataFrames and DataSets

** DataFrame

 - Extends from RDD
 - Contains row objects
 - Has a schema
 - Can run actual sql queries on it
 - Easy to communicate to relational db's
 - Schema inferred at runtime

** DataSet

Works best with structured data!

documentation: spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Dataset.html


 - A dataframe is really just a dataset of Row objects [DataSet[Row]]
 - Dataset knows schema at compile time
 - Datasets can only be used with Java and Scala (Python is not a compiled language)
 - RDD .toDS()
 - Can be more efficient, can be serialized very efficiently

*** SparkSession

Need a sparksession. Can get sparkcontext from sparksession

Note: Have to stop a session

To create a spark session:

#+BEGIN_SRC scala
  val spark = SparkSession
    .builder
    .appName("FriendsByAgeDS")
    .master("local[*]")
    .getOrCreate()

  //remember to stop at the end
  spark.stop()
#+END_SRC


*** Creating a dataset

first create a case class with the schema, then read in the data with "as[caseclass]"

If your data has headers, you can use option header true +
inferschema. If it doesn't, inferschema might fail and thus you need
to create an explicit schema and pass it in.

Extra part: add logging level as error

#+BEGIN_SRC scala
  import org.apache.spark.sql.SparkSession
  import org.apache.log4j._

  case class Person(id:Int, name:String, age:Int, friends:Long)

  //main function
  Logger.getLogger("org").setLevel(Level.ERROR)

  val ds = spark.read
    .option("header", "true")
    .option("inferSchema", "true")
    .csv("src/main/resources/fakefriends.csv")
    .as[Person]
    .cache()

  //Or use implicits to convert to dataset from dataframe

  import spark.implicits._
  val ds = spark.read.text("fakefriends.csv").as[Person]
#+END_SRC


Can also specify the schema as we're readying:

#+BEGIN_SRC scala
  import org.apache.spark.sql.types._

  val personSchema = new StructType()
        .add("id", StringType, nullable = true)
        .add("name", StringType, nullable = true)
        .add("age", IntegerType, nullable = true)
        .add("friends", LongType, nullable = true)

  val ds = spark.read
    .schema(personSchema)
    .csv("xx.csv")
    .as[Person]
#+END_SRC


*** Examples

#+BEGIN_SRC scala
  //show shows the first 20 rows by default. Pass in an integer to set how many rows to show
  ds.show()
  ds.select("someField")
  ds.filter(ds("someField")>200)
  ds.groupBy(ds("someField)).mean()
  ds.rdd().map(mapperFunction)
#+END_SRC

More advanced:

#+BEGIN_SRC scala
  import org.apache.spark.sql.functions._
  ds.groupBy("age").agg(round(avg("friends"), 2).alias("friends_avg")).sort("age").show()

  //note syntax, use $ to pass column names into sql functions, and weird =!= for not equal to
  ds.select(explode(split($"value", "\\W+")).alias("word")).filter($"word" =!= "")
#+END_SRC

Look at degreesOfSeperation.scala for example of using an accumulator to do a BFS 

*** Useful functions

  - withColumn (see git code MinTemperaturesDS) to add a new column


** SparkSQL

Exposes a jdbc / odbc server so you can use it just like a sql db

** UDF

spark.sql.funcitons.udf - can create user defined functions

* Running spark on a cluster

Before running:
 - make sure all the paths in the project don't point to your local drive
 - create jar file
 - use spark-submit <mainClass> --jars <dependencies> --files <filesToUse> <pathToJar>

** Set up local spark 

 - go to spark website, download zip file (using 3.0.0 with hadoop 2.7 in this course)
 - decompress zip file to somewhere you can find it again
 - create jar from project

*** Using SBT

Project should have the following subdirectory structure:

src - main - scala
project

Create assembly.sbt file inside the project folder with the following line:
addSbtPlugin("com.eed3si9n" % "sbt-assembly" % "0.14.10") 

Check latest sbt-assembly documentation as this will change over time

Create build.sbt:
(Spark 3 requires scala 2.12 - check scala version to spark version)

#+BEGIN_SRC sbt
name := "PopularMovies"

version := "1.0"

organization := "com.sundogsoftware"

scalaVersion := "2.12.3"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-core" % "3.0.0" % "provided"
)
#+END_SRC

provided: We assume the package will be pre-installed where we run it on, thus the package will not be included in the compiled jar file

Run: sbt assembly



