<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-10-21 Wed 21:47 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Ultimate Hands-on with Hadoop</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Mercia Malan" />
<link id="pagestyle" rel="stylesheet" type="text/css" href="../org.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="org-div-home-and-up">
 <a accesskey="h" href=""> UP </a>
 |
 <a accesskey="H" href="../index.html"> HOME </a>
</div><div id="content">
<h1 class="title">Ultimate Hands-on with Hadoop</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org00b4062">HDFS</a></li>
<li><a href="#org1ea4f74">MapReduce</a></li>
<li><a href="#org1b4d55d">Pig</a></li>
<li><a href="#org437b55d">Spark</a></li>
<li><a href="#org1956b2b">Hive</a></li>
<li><a href="#orge130a1a">Sqoop</a></li>
<li><a href="#orgfd33343">Integrating hadoop with nosql</a></li>
<li><a href="#org7de8743">Querying data interactively</a></li>
<li><a href="#orgf7fb202">Manage your cluster</a></li>
<li><a href="#org5b7606f">Feeding data into your cluster</a></li>
<li><a href="#orgb37771d">Analyzing streams of data</a></li>
<li><a href="#orgd7c56f3">Designing Real-world systems</a></li>
</ul>
</div>
</div>


<div class="figure">
<p><img src="./hadoop.png" alt="hadoop.png" width="800" />
</p>
<p><span class="figure-number">Figure 1: </span>Hadoop Eco System</p>
</div>


<dl class="org-dl">
<dt>HDFS</dt><dd>Distributed file system, manages data that stores it in blocks distributed across the system</dd>
<dt>YARN</dt><dd>Manages computation of your cluster (Yet another resource negotiator), what gets run where</dd>
<dt>MapReduce</dt><dd>Transform and aggregate data</dd>
<dt>Hive</dt><dd>SQL Like queries on your cluster</dd>
<dt>Pig</dt><dd>Scripting interface using pig-latin</dd>
<dt>Tez</dt><dd>DAG alternative to MapReduce. Hive and Pig has alternative interfaces for Tez</dd>
<dt>Spark</dt><dd>DAGs and in-memory processing. Ecosystem - streaming, graphs etc</dd>
<dt>Apache HBase</dt><dd>NoSQL BigTable - favours consistency</dd>
<dt>MySQL, MongoDB, Cassandra</dt><dd>Other databases</dd>
<dt>Sqoop</dt><dd>Import and export data from mysql into hadoop</dd>
<dt>Zookeeper</dt><dd>Keep track of who is the current master</dd>
<dt>Oozie</dt><dd>workflows and schedule jobs across cluster</dd>
<dt>Querie Engines</dt><dd>Apache Drill, Phoenix, Presto - allows SQL query writing across multiple databases</dd>
<dt>Hue</dt><dd>Cloudera's answer to Ambari , web ui</dd>
<dt>Apache Zeppelin</dt><dd>Notebook style writing</dd>
<dt>Streaming</dt><dd>Storm, spark streaming, flink</dd>
<dt>Flume, Kafka</dt><dd>Streams data into your cluster at high transaction rates</dd>
<dt>Ambari</dt><dd>In hortonworks gives web interface to manage everything</dd>
</dl>

<div id="outline-container-org00b4062" class="outline-2">
<h2 id="org00b4062">HDFS</h2>
<div class="outline-text-2" id="text-org00b4062">
<p>
Optimised for large files, not really many small files
</p>

<p>
Stores data in blocks, and these blocks are stored across multiple servers. Data is stored in more than one block for fail-over
</p>

<p>
Single Name Node - keeps track of where all blocks live. Knows the file name and path. Contains an edit block so it knows what has been created and edited.
Data Nodes - contains data
</p>
</div>

<div id="outline-container-org2fbdbd6" class="outline-3">
<h3 id="org2fbdbd6">Reading a file</h3>
<div class="outline-text-3" id="text-org2fbdbd6">
<p>
Client node talks to Name node to see which blocks to go look at, then client node goes to the actual data nodes.
</p>
</div>
</div>

<div id="outline-container-orgc607b8f" class="outline-3">
<h3 id="orgc607b8f">Writing a file</h3>
<div class="outline-text-3" id="text-orgc607b8f">
<p>
Client node tells name node the file node.
Client talks to single data node to write file, and data nodes will talk to each other to store the data in replicated manner
Data nodes send back a acknowledged and client node tells name node where it was stored and that it has been written
</p>
</div>
</div>

<div id="outline-container-org290e700" class="outline-3">
<h3 id="org290e700">Name node DR</h3>
<div class="outline-text-3" id="text-org290e700">
<p>
Always only have 1 active Name node, several options to avoid losing data:
</p>

<ul class="org-ul">
<li>backup. In case of name node failure, bring up new name node with backups</li>
<li>Secondary namenode. Maintains merged copy of edit log you can restore from.</li>
<li>HDFS Federation. Each namenode manages a specific namespace volume. Separate name nodes for separate volumes (Namenode can reach name list limit)</li>
<li>HDFS High Available. If you can't afford downtime. Uses a shared edit log. Zookeeper tracks active namenode and extreme measures to ensure only one namenode is used at a time</li>
</ul>
</div>
</div>

<div id="outline-container-org6c1360d" class="outline-3">
<h3 id="org6c1360d">Using HDFS</h3>
<div class="outline-text-3" id="text-org6c1360d">
<ul class="org-ul">
<li>UI (Ambari) - looks like a giant file system</li>
<li>CLI</li>
<li>HTTP directly or via HDFS Proxies</li>
<li>Java interface</li>
<li>NFS Gateway (network file system), way of remoting a remote file system on the server. Will just look like another mount point on your linux box.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org1ea4f74" class="outline-2">
<h2 id="org1ea4f74">MapReduce</h2>
<div class="outline-text-2" id="text-org1ea4f74">
<p>
2 stages: Map and Reduce.
</p>

<p>
Map phase includes shuffling and sorting. 
</p>

<p>
Example using python. Can run locally with python script.py u.data 
or on hadoop cluster with python scipt.py -r hadoop &#x2013;hadoop-streaming-jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar u.data
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> mrjob.job <span style="color: #51afef;">import</span> MRJob
<span style="color: #51afef;">from</span> mrjob.step <span style="color: #51afef;">import</span> MRStep

<span style="color: #51afef;">class</span> <span style="color: #ECBE7B;">RatingsBreakdown</span>(MRJob):
    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">steps</span>(<span style="color: #51afef;">self</span>):
        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">Takes in 2 parameters - a mapper and a reducer</span>
        <span style="color: #5B6268;"># </span><span style="color: #5B6268;">in this case the mapper is the output of a mapper and reducer</span>
        <span style="color: #51afef;">return</span> [
            MRStep(mapper=<span style="color: #51afef;">self</span>.mapper_get_ratings,
                   reducer=<span style="color: #51afef;">self</span>.reducer_count_ratings),
            MRStep(reducer=<span style="color: #51afef;">self</span>,reducer_sorted_output)
            ]

    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">mapper_get_ratings</span>(<span style="color: #51afef;">self</span>, _, line):
        (userID, movieID, rating, timestamp) = line.split(<span style="color: #98be65;">'\t'</span>)
        <span style="color: #51afef;">yield</span> movieID, 1

    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">reducer_count_ratings</span>(<span style="color: #51afef;">self</span>, key, values):
        <span style="color: #51afef;">yield</span> <span style="color: #c678dd;">str</span>(<span style="color: #c678dd;">sum</span>(values)).zfill(5), key <span style="color: #5B6268;">#</span><span style="color: #5B6268;">zfill filss the number up to 5 spaces e.g. 00004</span>

    <span style="color: #51afef;">def</span> <span style="color: #c678dd;">reducer_sorted_output</span>(<span style="color: #51afef;">self</span>, count, movies):
        <span style="color: #51afef;">for</span> movie <span style="color: #51afef;">in</span> movies:
            <span style="color: #51afef;">yield</span> movie, count


<span style="color: #51afef;">if</span> <span style="color: #c678dd;">__name__</span> == <span style="color: #98be65;">'__main__'</span>:
    RatingsBreakdown.run()
</pre>
</div>
</div>
</div>

<div id="outline-container-org1b4d55d" class="outline-2">
<h2 id="org1b4d55d">Pig</h2>
<div class="outline-text-2" id="text-org1b4d55d">
<p>
Built on top of MapReduce or Tez, a SQL-like interface (Pig Latin) to write MapReduce pipelines without having to use the complex MapReduce
</p>

<p>
Can run this using grunt, script or ambarai / hue
</p>

<p>
Example on MapReduce: load movie data, transform to show average ratings, filter to show only ratings &gt; 4, join datasets to get names of movies and not ID's.
To execute on Tez (faster): check the "execute on Tez" checkbox and execute.
(Tez uses acyclic graphs for computation)
</p>

<p>
Each of the expressions is called a 'relation'
</p>

<div class="org-src-container">
<pre class="src src-text">ratings = LOAD '/user/maria_dev/ml-100k/u.data' AS (userId:int, movieId:int, rating:int, ratingTime:int);

metadata = LOAD '/user/maria_dev/ml-100k/u.item' USING PigStorage('|')
    AS (movieID:int, movieTitle:chararray, releaseDate:chararray, videoRelease:chararray, imdbLink:chararray);

nameLookup = FOREACH metadata GENERATE movieID, movieTitle,
    ToUnixTime(ToDate(releaseDate, 'dd-MMM-yyyy')) AS releaseTime;

ratingsByMovie = GROUP ratings BY movieId;

avgRatings = FOREACH ratingsByMovie GENERATE group AS movieID, AVG(ratings.rating) AS avgRating;

fiveStarMovies = FILTER avgRatings BY avgRating &gt; 4.0;

fiveStarsWithData = JOIN fiveStarMovies BY  movieID, nameLookup BY movieID;

oldestFiveStarMovies = ORDER fiveStarsWithData BY nameLookup::releaseTime;

DUMP oldestFiveStarMovies;
</pre>
</div>
</div>

<div id="outline-container-org486cd28" class="outline-3">
<h3 id="org486cd28">Basic commands</h3>
<div class="outline-text-3" id="text-org486cd28">
<ul class="org-ul">
<li>LOAD STORE DUMP</li>
<li>STORE ratings INTO 'outRatings' USING PigStorage(':');</li>
<li>FILTER DISTINCT FOREACH/GENERATE MAPREDUCE STREAM STREAM SAMPLE</li>
<li>JOIN COGROUP GROUP CUBE</li>
<li>ORDER RANK LIMIT</li>
<li>UNION SPLIT</li>
</ul>
</div>
</div>

<div id="outline-container-orgea66f77" class="outline-3">
<h3 id="orgea66f77">Diagnostics</h3>
<div class="outline-text-3" id="text-orgea66f77">
<ul class="org-ul">
<li>DESCRIBE</li>
<li>EXPLAIN</li>
<li>ILLUSTRATE</li>
</ul>
</div>
</div>

<div id="outline-container-org652f900" class="outline-3">
<h3 id="org652f900">UDF's</h3>
<div class="outline-text-3" id="text-org652f900">
<ul class="org-ul">
<li>REGISTER</li>
<li>DEFINE</li>
<li>IMPORT</li>
</ul>
</div>
</div>

<div id="outline-container-orge32006d" class="outline-3">
<h3 id="orge32006d">Some other functions and loaders</h3>
<div class="outline-text-3" id="text-orge32006d">
<ul class="org-ul">
<li>AVG CONCAT COUNT MAX MIN SIZE SUM</li>
<li>PigStorage</li>
<li>TextLoader</li>
<li>JsonLoader</li>
<li>AvroStorage</li>
<li>ParquetLoader</li>
<li>OrcStorage</li>
<li>HBaseStorage</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org437b55d" class="outline-2">
<h2 id="org437b55d">Spark</h2>
<div class="outline-text-2" id="text-org437b55d">
<p>
DAG Engine (directed acyclic graph) optimizes workflows
</p>

<p>
Components of spark part of spark core
</p>

<ul class="org-ul">
<li>Spark streaming</li>
<li>Spark SQL</li>
<li>MLLib</li>
<li>GraphX</li>
</ul>

<p>
ssh into hadoop cluster, run: spark-submit file.py
</p>

<p>
Can start a thrift service with spark sql and connect to it and query it
</p>

<p>
More details in spark course
</p>
</div>
</div>

<div id="outline-container-org1956b2b" class="outline-2">
<h2 id="org1956b2b">Hive</h2>
<div class="outline-text-2" id="text-org1956b2b">
<p>
On top of Mapreduce and Tez. 
</p>

<p>
Short for HiveQL - allows you to query HDFS data using SQL syntax
</p>

<p>
Basically smokes and mirrors to make it seem like you're working with a relational database
</p>
</div>

<div id="outline-container-org987cd47" class="outline-3">
<h3 id="org987cd47">Why not hive?</h3>
<div class="outline-text-3" id="text-org987cd47">
<ul class="org-ul">
<li>high latency - not appropriate for OLTP</li>
<li>stores data de-normalized</li>
<li>SQL is limited in what it can do (Pig, spark allows more complex stuff)</li>
<li>No transactions</li>
<li>no record-level updates, inserts, deletes</li>
</ul>

<p>
In hive view, can upload table and then write queries in HQL.
</p>

<p>
Can create views as well (which gets persisted as with usual relational db)
</p>
</div>
</div>

<div id="outline-container-org3f43711" class="outline-3">
<h3 id="org3f43711">How does hive work?</h3>
<div class="outline-text-3" id="text-org3f43711">
<p>
Schema on read
</p>

<p>
Hive takes unstructured data and applies a schema to it as it reads, where relational databases write the schema first and read data according to that schema (schema on write)
</p>

<p>
 LOAD DATA - hive will move data from a distributed filesystem into Hive (the raw data)
 LOAD DATA LOCAL - copies data from local filesystem into Hive
 Managed vs External tables: managed tables are where hive takes control of that data. 
To create external table use "CREATE EXTERNAL TABLE", give it a location and then hive doesn't take ownership of it. Thus, dropping data will drop metadata but not the actual data.
</p>
</div>

<div id="outline-container-org251c918" class="outline-4">
<h4 id="org251c918">Partitioning</h4>
<div class="outline-text-4" id="text-org251c918">
<p>
You can store your data in partitioned subdirectories (optimisation)
</p>

<p>
E.g. 
</p>
<div class="org-src-container">
<pre class="src src-sql"> <span style="color: #51afef;">CREATE</span> <span style="color: #51afef;">TABLE</span> <span style="color: #c678dd;">person</span>(
   <span style="color: #51afef;">name</span> STRING,
   address STRUCT&lt;street: String, city: String&gt;
 )
PARTITIONED <span style="color: #51afef;">BY</span> (country STRING)
</pre>
</div>

<p>
Can use it through Ambari / Hue; JDBC/ODBC server; Thrift service (but remember hive is not suitable for OLTP); via Oozie
</p>
</div>
</div>
</div>
</div>



<div id="outline-container-orge130a1a" class="outline-2">
<h2 id="orge130a1a">Sqoop</h2>
<div class="outline-text-2" id="text-orge130a1a">
<p>
Meant for large datasets.
</p>

<p>
Kicks of mapreduce jobs to handle importing and exporting your data
</p>

<p>
Takes data from mysql/postgres etc, distributes processing across several parallel mappers and writing to HDFS.
</p>

<p>
Command line tool:
</p>

<div class="org-src-container">
<pre class="src src-bash">sqoop import --connect jdbc:mysql://localhost/movielens --driver com.mysql.jdbc.Driver --table movies
</pre>
</div>

<p>
To add to hive instead add '&#x2013;hive-import'
To set the number of mappers add -m 1
</p>

<p>
Can do incremental imports in sqoop (can be used to keep table up to date) by using &#x2013;check-column (to check like a date column) and &#x2013;last-value 
</p>

<p>
To export from hive table to mysql: (mysql table needs to exist)
</p>

<div class="org-src-container">
<pre class="src src-bash">sqoop export --connect jdbc:mysql://localhost/movielens -m 1 --driver com.mysql.jdbc.Driver --table exported_movies --export-dir /apps/hive/warehouse/movies --input-fields-terminated-by <span style="color: #98be65;">'\0001'</span>
</pre>
</div>
</div>
</div>

<div id="outline-container-orgfd33343" class="outline-2">
<h2 id="orgfd33343">Integrating hadoop with nosql</h2>
<div class="outline-text-2" id="text-orgfd33343">
<p>
CAP: Consistency, Availability, Partition-Tolerance
</p>

<p>
Mysql = C/A
cassandra = A/P
Hbase and mongodb = C/P
</p>
</div>

<div id="outline-container-org98f3611" class="outline-3">
<h3 id="org98f3611">HBase</h3>
<div class="outline-text-3" id="text-org98f3611">
<p>
HBase is built on top of hdfs, based on google's BigTable
</p>

<p>
Does not have a query language but has a CRUD API's
</p>

<p>
Auto-sharding on top of HDFS onto "region servers"
</p>

<p>
HMaster (master nodes that keeps track of where which data is)
Zookeeper is the "who is watching the watchers", keeps track of where the Master server is and its status.
</p>

<p>
HBase is transactional on rows
</p>
</div>

<div id="outline-container-org43e042c" class="outline-4">
<h4 id="org43e042c">HBase data model</h4>
<div class="outline-text-4" id="text-org43e042c">
<p>
keys stored lexographically in hbase 
</p>

<p>
ROW referenced by a unique KEY
Each ROW has some small number of COLUMN FAMILIES which may contain arbitrary COLUMNS.
</p>

<p>
E.g. if you have ratings you'll have a column family for ratings, and the family has columns that may or may not be filled in
</p>

<p>
CELL: intersection of a row and a column, and each cell can have many versions with given timestamps
</p>


<p>
Example:
</p>

<p>
key: com.cnn.www
Contents Column family: contents (one column with multiple versions - history of the webpage)
Anchor Column family: Anchor: cnnsi.com = "CNN"; Anchor:my.look.ca = "CNN.com"   &gt;&gt; syntax: key = columnFamily:Name value = whatever value
</p>
</div>
</div>

<div id="outline-container-org768df81" class="outline-4">
<h4 id="org768df81">Access HBase</h4>
<div class="outline-text-4" id="text-org768df81">
<ul class="org-ul">
<li>HBase shell</li>
<li>Java API (wrappers for python, scala etc)</li>
<li>Spark, Hive, Pig</li>
<li>REST service</li>
<li>Thrift service</li>
<li>Avro service</li>
</ul>
</div>
</div>

<div id="outline-container-org5ee4c8c" class="outline-4">
<h4 id="org5ee4c8c">Examples</h4>
<div class="outline-text-4" id="text-org5ee4c8c">
<p>
HBASE table:
</p>

<p>
UserID; Rating:50; Rating:33; Rating:233  # rating for movie 50 was e.g. 1 star (so value = 1)
</p>
</div>

<div id="outline-container-org73868a4" class="outline-5">
<h5 id="org73868a4">Start HBASE through admin</h5>
</div>

<div id="outline-container-org82b9ac8" class="outline-5">
<h5 id="org82b9ac8">Kick off rest server running on top of HBASE</h5>
<div class="outline-text-5" id="text-org82b9ac8">
<p>
log into cluster via ssh
</p>

<p>
start and stop
</p>
<div class="org-src-container">
<pre class="src src-bash"> /usr/hdp/current/hbase-master/bin/hbase-daemon.sh start rest -p 8000 --infoport 8001 
/usr/hdp/current/hbase-master/bin/hbase-daemon.sh stop rest 
</pre>
</div>
</div>
</div>


<div id="outline-container-org1e0f19d" class="outline-5">
<h5 id="org1e0f19d">Do stuff to rest using python</h5>
<div class="outline-text-5" id="text-org1e0f19d">
<p>
Use library called starbase
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> starbase <span style="color: #51afef;">import</span> Connection

<span style="color: #dcaeea;">c</span> = Connection(<span style="color: #98be65;">"sandbox-hdp.hortonworks.com"</span>, <span style="color: #98be65;">"8000"</span>)

<span style="color: #dcaeea;">ratings</span> = c.table(<span style="color: #98be65;">'ratings'</span>)

<span style="color: #51afef;">if</span>(ratings.exists()):
    <span style="color: #51afef;">print</span>(<span style="color: #98be65;">"Dropping existing ratings table\n"</span>)
    ratings.drop()

ratings.create(<span style="color: #98be65;">'rating'</span>)

<span style="color: #51afef;">print</span>(<span style="color: #98be65;">"Parsing data\n"</span>)
<span style="color: #dcaeea;">ratingFile</span> = <span style="color: #c678dd;">open</span>(<span style="color: #98be65;">'/opt/jemstep/code/udemy/ml-100k/u.data'</span>, <span style="color: #98be65;">'r'</span>)

<span style="color: #dcaeea;">batch</span> = ratings.batch()

<span style="color: #51afef;">for</span> line <span style="color: #51afef;">in</span> ratingFile: 
    (userID, movieID, rating, timestamp) = line.split()
    batch.update(userID, {<span style="color: #98be65;">'rating'</span>: {movieID: rating}})

ratingFile.close()

<span style="color: #51afef;">print</span>(<span style="color: #98be65;">"Committing ratings data to HBase via REST service\n"</span>)
batch.commit(finalize=<span style="color: #a9a1e1;">True</span>)

<span style="color: #51afef;">print</span>(<span style="color: #98be65;">"Fetch data. Ratings for user ID 1\n"</span>)
<span style="color: #51afef;">print</span>(ratings.fetch(<span style="color: #98be65;">"1"</span>))
<span style="color: #51afef;">print</span>(<span style="color: #98be65;">"Ratings for user ID 33\n"</span>)
<span style="color: #51afef;">print</span>(ratings.fetch(<span style="color: #98be65;">"33"</span>))
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-org44e05cc" class="outline-4">
<h4 id="org44e05cc">Integrating Pig with HBase</h4>
<div class="outline-text-4" id="text-org44e05cc">
<ul class="org-ul">
<li>Must create HBase table ahead of time</li>
<li>Your relation must have a unique key as its first column, followed by subsequent columns as you want htem saved in HBase</li>
<li>USING clause allows you to STORE into an HBase table</li>
<li>Can work at scale because HBase is transactional on rows</li>
</ul>

<p>
To create a new table in hbase:
</p>

<div class="org-src-container">
<pre class="src src-bash">hbase shell

list <span style="color: #5B6268;">#</span><span style="color: #5B6268;">shows tables</span>

create <span style="color: #98be65;">'users'</span>, <span style="color: #98be65;">'userinfo'</span> <span style="color: #5B6268;"># </span><span style="color: #5B6268;">create table user with one column family 'userinfo'</span>

scan <span style="color: #98be65;">'users'</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">peeks into table - timestamp built-in in result because hbase is versioned</span>

<span style="color: #c678dd;">disable</span> <span style="color: #98be65;">'users'</span> <span style="color: #5B6268;">#</span><span style="color: #5B6268;">need to do this before being able to drop</span>
drop <span style="color: #98be65;">'users'</span>

</pre>
</div>

<p>
Create a pig file and run using 'pig file.pig'
</p>

<div class="org-src-container">
<pre class="src src-sql">ratings = LOAD <span style="color: #98be65;">'/xxx/u.user'</span>
<span style="color: #51afef;">USING</span> PigStorage(<span style="color: #98be65;">'|'</span>)
<span style="color: #51afef;">AS</span> (userID:<span style="color: #ECBE7B;">int</span>, age:<span style="color: #ECBE7B;">int</span>, gender:chararray, occupation:chararray, zip:<span style="color: #ECBE7B;">int</span>);

STORE ratings <span style="color: #51afef;">INTO</span> <span style="color: #98be65;">'hbase://users'</span>
<span style="color: #51afef;">USING</span> org.apache.pig.backend.hadoop.hbase.HBaseStorage (
<span style="color: #98be65;">'userinfo:age,userinfo:gender,userinfo:occupation,userinfo:zip'</span>);
</pre>
</div>
</div>
</div>
</div>


<div id="outline-container-org8bf4b90" class="outline-3">
<h3 id="org8bf4b90">Cassandra</h3>
<div class="outline-text-3" id="text-org8bf4b90">
</div>
<div id="outline-container-orgd128c3c" class="outline-4">
<h4 id="orgd128c3c">How did it start?</h4>
<div class="outline-text-4" id="text-orgd128c3c">
<p>
comes from greek mythology - cassandra can tell the future
</p>

<p>
CAP theorem: consistency, availability and partition-tolerance, they say you can only have 2 out of 3.
</p>

<p>
Cassandra favors availability over consistency (eventually consistent), but you can tune it so "tunable consistency"
</p>
</div>
</div>

<div id="outline-container-orge85ba47" class="outline-4">
<h4 id="orge85ba47">What is it?</h4>
<div class="outline-text-4" id="text-orge85ba47">
<p>
Distributed nosql with no single point of failure
</p>

<p>
No master node, every node runs exactly the same software and performs the same functions
</p>

<p>
Data model similar to hbase
</p>

<p>
non-relational but has limited CQL query language
</p>
</div>
</div>

<div id="outline-container-org675cc51" class="outline-4">
<h4 id="org675cc51">Cassandra architecture</h4>
<div class="outline-text-4" id="text-org675cc51">
<p>
Ring architecture between all nodes for high availability
</p>

<p>
Nodes talk to each other and manage themselves
</p>

<p>
Can have multiple cassandra rings and replicate between. So you can use your replica ring for analytics without impacting transactional performance
</p>
</div>
</div>

<div id="outline-container-org4243f52" class="outline-4">
<h4 id="org4243f52">CQL</h4>
<div class="outline-text-4" id="text-org4243f52">
<p>
no joins, all queries must be on some primary key
</p>

<p>
data has to be de-normalized
</p>

<p>
all tables must be in a keyspace (keyspaces are like databases)
</p>

<p>
Can also use CQLSH which is CQL in shell to create tables and stuff
Replication should ideally be higher than below example, but for local with one node this is fine.
</p>
<div class="org-src-container">
<pre class="src src-sql">cqlsh <span style="color: #5B6268;">--cqlversion="3.4.0"</span>
<span style="color: #51afef;">CREATE</span> KEYSPACE movielens <span style="color: #51afef;">WITH</span> replication = {<span style="color: #98be65;">'class'</span>= <span style="color: #98be65;">'SimpleStrategy'</span>, <span style="color: #98be65;">'replication_factor'</span>:<span style="color: #98be65;">'1'</span>} <span style="color: #51afef;">AND</span> durable_writes = <span style="color: #51afef;">true</span>; 
USE movielens
<span style="color: #51afef;">CREATE</span> <span style="color: #51afef;">TABLE</span> <span style="color: #c678dd;">users</span> (user_id <span style="color: #ECBE7B;">int</span>, age <span style="color: #ECBE7B;">int</span>, <span style="color: #51afef;">PRIMARY</span> <span style="color: #51afef;">KEY</span> (user_id))
<span style="color: #51afef;">DESCRIBE</span> <span style="color: #51afef;">TABLE</span> users

</pre>
</div>
</div>
</div>

<div id="outline-container-orgaaaa574" class="outline-4">
<h4 id="orgaaaa574">Cassandra + Spark</h4>
<div class="outline-text-4" id="text-orgaaaa574">
<p>
DataStax has a spark-cassandra connector, which allows you to RW as dataframes         
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #5B6268;"># </span><span style="color: #5B6268;">to set spark version, set env var SPARK_MAJOR_VERSION</span>

<span style="color: #51afef;">from</span> pyspark.sql <span style="color: #51afef;">import</span> SparkSession
<span style="color: #51afef;">from</span> pyspark.sql <span style="color: #51afef;">import</span> Row
<span style="color: #51afef;">from</span> pyspark.sql <span style="color: #51afef;">import</span> functions

<span style="color: #51afef;">def</span> <span style="color: #c678dd;">parseInput</span>(line):
    <span style="color: #dcaeea;">fields</span> = line.split(<span style="color: #98be65;">'|'</span>)
    <span style="color: #51afef;">return</span> Row(user_id = <span style="color: #c678dd;">int</span>(fields[0]), age = <span style="color: #c678dd;">int</span>(fields[1]))

<span style="color: #51afef;">if</span> __name-_ == <span style="color: #98be65;">"__main__"</span>:
    <span style="color: #dcaeea;">spark</span> = SparkSession.builder.appName(<span style="color: #98be65;">"Cass"</span>).config(<span style="color: #98be65;">"spark.cassandra.connection.host"</span>,<span style="color: #98be65;">"127.0.0.1"</span>).getOrCreate()
    <span style="color: #dcaeea;">lines</span> = spark.sparkContext.textFile(<span style="color: #98be65;">"hdfs:///user/maria_dev/ml-100k/u.user"</span>)

     <span style="color: #dcaeea;">users</span> = lines.<span style="color: #c678dd;">map</span>(parseInput)

     <span style="color: #dcaeea;">usersDf</span> = spark.createDataFrame(users)

     usersDf.write\
         .<span style="color: #c678dd;">format</span>(<span style="color: #98be65;">"org.apache.spark.sql.cassandra"</span>)\
         .mode(<span style="color: #98be65;">'append'</span>)\
         .options(table=<span style="color: #98be65;">"users"</span>, keyspace=<span style="color: #98be65;">"movielens"</span>)\
         .save()

     <span style="color: #dcaeea;">readUsers</span> = spark.read\
         .<span style="color: #c678dd;">format</span>(<span style="color: #98be65;">"org.apache.spark.sql.cassandra"</span>)\
         .options(table=<span style="color: #98be65;">"users"</span>, keyspace=<span style="color: #98be65;">"movielens"</span>)\
         .load()

     readUsers.createOrReplaceTempView(<span style="color: #98be65;">"users"</span>)

     <span style="color: #dcaeea;">sqlDF</span> = spark.sql(<span style="color: #98be65;">"SELECT * FROM users WHERE age &lt; 20"</span>)
     sqlDF.show()

     spark.stop()
</pre>
</div>

<p>
To run: (from hdp sandbox shell)
spark-submit &#x2013;packages datastax:spark-cassandra-connector:2.0.0-M2-s_2.11 CassandraSpark.py
</p>
</div>
</div>
</div>


<div id="outline-container-orgf6c97b8" class="outline-3">
<h3 id="orgf6c97b8">MongoDB</h3>
<div class="outline-text-3" id="text-orgf6c97b8">
<p>
Nothign new&#x2026;
</p>
</div>

<div id="outline-container-org5343b73" class="outline-4">
<h4 id="org5343b73">MongoDB and Spark</h4>
<div class="outline-text-4" id="text-org5343b73">
<p>
Can add mongo as a service to ambari - install via hdp sandbox shell, then add new service @ ambari and select it.
</p>

<p>
Python script:
</p>

<div class="org-src-container">
<pre class="src src-python"><span style="color: #51afef;">from</span> pyspark.sql <span style="color: #51afef;">import</span> SparkSession
 <span style="color: #51afef;">from</span> pyspark.sql <span style="color: #51afef;">import</span> Row
 <span style="color: #51afef;">from</span> pyspark.sql <span style="color: #51afef;">import</span> functions

 <span style="color: #51afef;">def</span> <span style="color: #c678dd;">parseInput</span>(line):
     <span style="color: #dcaeea;">fields</span> = line.split(<span style="color: #98be65;">'|'</span>)
     <span style="color: #51afef;">return</span> Row(user_id = <span style="color: #c678dd;">int</span>(fields[0]), age = <span style="color: #c678dd;">int</span>(fields[1]))

 <span style="color: #51afef;">if</span> __name-_ == <span style="color: #98be65;">"__main__"</span>:
     <span style="color: #dcaeea;">spark</span> = SparkSession.builder.appName(<span style="color: #98be65;">"mongo"</span>).getOrCreate()
     <span style="color: #dcaeea;">lines</span> = spark.sparkContext.textFile(<span style="color: #98be65;">"hdfs:///user/maria_dev/ml-100k/u.user"</span>)

      <span style="color: #dcaeea;">users</span> = lines.<span style="color: #c678dd;">map</span>(parseInput)

      <span style="color: #dcaeea;">usersDf</span> = spark.createDataFrame(users)

      usersDf.write\
          .<span style="color: #c678dd;">format</span>(<span style="color: #98be65;">"com.mongodb.spark.sql.DefaultSource"</span>)\
          .mode(<span style="color: #98be65;">'append'</span>)\
          .option(<span style="color: #98be65;">"uri"</span>,<span style="color: #98be65;">"mongodb://127.0.0.1/movielens.users"</span>)\
          .save()

      <span style="color: #dcaeea;">readUsers</span> = spark.read\
          .<span style="color: #c678dd;">format</span>(<span style="color: #98be65;">"com.mongodb.spark.sql.DefaultSource"</span>)\
          .option(<span style="color: #98be65;">"uri"</span>,<span style="color: #98be65;">"mongodb://127.0.0.1/movielens.users"</span>)\
          .load()

      readUsers.createOrReplaceTempView(<span style="color: #98be65;">"users"</span>)

      <span style="color: #dcaeea;">sqlDF</span> = spark.sql(<span style="color: #98be65;">"SELECT * FROM users WHERE age &lt; 20"</span>)
      sqlDF.show()

      spark.stop()


</pre>
</div>

<p>
run:
</p>

<p>
spark-submit &#x2013;packages org.mongodb.spark:mongo-spark-connector 2.11:2.0.0 mongospark.py
</p>
</div>
</div>
</div>


<div id="outline-container-orga0a9dae" class="outline-3">
<h3 id="orga0a9dae">So how do you choose?</h3>
<div class="outline-text-3" id="text-orga0a9dae">
<p>
Integration consideration, e.g. if you're using spark, you probably want to choose a db that can work with spark.
</p>

<p>
Scaling requirements, do you need distributed storing and processing?
</p>

<p>
Support considerations - do you have the expertise needed to support the tech?
</p>

<p>
Budget considerations
</p>

<p>
CAP considerations
</p>

<p>
Keep it simple, if you don't need to setup a complex nosql cluster, don't do it if you don't need to.
</p>
</div>
</div>
</div>

<div id="outline-container-org7de8743" class="outline-2">
<h2 id="org7de8743">Querying data interactively</h2>
<div class="outline-text-2" id="text-org7de8743">
<p>
Query engines:
</p>
<ul class="org-ul">
<li>Drill [ SQL engine that allows you to run sql queries on non-relational databases and data files ]</li>
<li>Hue</li>
<li>Phoenix [ SQL driver for HBase ]</li>
<li>Presto [ Distributing queries across different data stores ] (meant for analysis, not for fast queries)</li>
<li>Apache Zeppelen</li>
</ul>
</div>
</div>

<div id="outline-container-orgf7fb202" class="outline-2">
<h2 id="orgf7fb202">Manage your cluster</h2>
<div class="outline-text-2" id="text-orgf7fb202">
</div>
<div id="outline-container-org6c73634" class="outline-3">
<h3 id="org6c73634">Yarn</h3>
<div class="outline-text-3" id="text-org6c73634">
<p>
Yet Another Resources Negotiator
</p>

<p>
HDFS is the cluster storage layer
YARN is the cluster compute layer on top of the storage layer
MapReduce, Spark and Tez sits on top of YARN and are YARN applications
</p>
</div>
</div>

<div id="outline-container-org58f7237" class="outline-3">
<h3 id="org58f7237">Tez</h3>
<div class="outline-text-3" id="text-org58f7237">
<p>
Makes Hive, Pig and MapReduce jobs faster
Constructs DAGs
Optimizes physical data flow and resource usage
</p>


<p>
MapReduce / Spark / Tez (YARN Applications) 
           YARN         (compute)
           HDFS         (storage)
</p>
</div>
</div>

<div id="outline-container-orgdb3393f" class="outline-3">
<h3 id="orgdb3393f">Mesos</h3>
<div class="outline-text-3" id="text-orgdb3393f">
<p>
Came out of Twitter - managers resources across your data center
</p>

<p>
More general, not just hadoop
</p>

<p>
Spark and Storm can run on mesos instead of yarn.
</p>

<p>
Can integrate yarn and mesos using myriad
</p>

<p>
Spark on mesos is limited to one executor per slave
</p>
</div>
</div>

<div id="outline-container-org7dcfb7a" class="outline-3">
<h3 id="org7dcfb7a">Zookeeper</h3>
<div class="outline-text-3" id="text-org7dcfb7a">
<p>
Coordinate your cluster
</p>

<p>
Keeps track of information that must be synchhronized across your cluster - which node is the master, which tasks are assigned to which workers, which workers are available
</p>

<p>
good idea to have zookeeper ensemble ("who's watching the watcher?")
</p>
</div>


<div id="outline-container-org9192884" class="outline-4">
<h4 id="org9192884">Failure modes</h4>
<div class="outline-text-4" id="text-org9192884">
<p>
zookeeper can help with some failures: 
</p>

<ul class="org-ul">
<li>master crashes - needs to fail over to a backup</li>
<li>worker crashes - work needs to be distributed</li>
<li>network trouble - part of your cluster can't see the rest of it (zookeeper notifies you)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orge3607fd" class="outline-3">
<h3 id="orge3607fd">Oozie</h3>
<div class="outline-text-3" id="text-orge3607fd">
<p>
Orchestrates your hadoop jobs (scheduling and running tasks)
</p>

<p>
Oozie = Burmese word for "Elephant Keeper"
</p>
</div>

<div id="outline-container-org33744ca" class="outline-4">
<h4 id="org33744ca">Workflows</h4>
<div class="outline-text-4" id="text-org33744ca">
<p>
Has a workflow concept which is a multi-stage hadoop job to chain together mapreduce, hive, pig, sqoop and distcp tasks. Can chain others like spark with add-ons.
DAG specified by XML
</p>

<p>
Nodes:
</p>
<ul class="org-ul">
<li>Start</li>
<li>End</li>
<li>Fork</li>
<li>Join</li>
</ul>



<div class="figure">
<p><img src="./oozie_workflow.png" alt="Example workflow" width="600" />
</p>
<p><span class="figure-number">Figure 2: </span>Example Workflow</p>
</div>

<p>
Example: Start -&gt; fork -&gt; pig ; sqoop -&gt; join -&gt; hive -&gt; End
</p>

<div class="org-src-container">
<pre class="src src-xml">&lt;?<span style="color: #51afef;">xml</span><span style="color: #83898d;">&gt;</span>

<span style="color: #83898d;">&lt;workflow-app &gt;</span>
<span style="color: #83898d;">  &lt;start to=</span><span style="color: #83898d;">"fork-node"</span><span style="color: #83898d;"> /&gt;</span>

<span style="color: #83898d;">  &lt;fork name=</span><span style="color: #83898d;">"fork-node"</span><span style="color: #83898d;">&gt;</span>
<span style="color: #83898d;">    &lt;path start=</span><span style="color: #83898d;">"sqoop-node"</span><span style="color: #83898d;"> /&gt;</span>
<span style="color: #83898d;">    &lt;path start=</span><span style="color: #83898d;">"pig-node"</span><span style="color: #83898d;"> /&gt;</span>
<span style="color: #83898d;">  &lt;/fork&gt;</span>

<span style="color: #83898d;">  &lt;action name=</span><span style="color: #83898d;">"sqoop-node"</span><span style="color: #83898d;">&gt;</span>
<span style="color: #83898d;">    &lt;sqoop xmlns=</span><span style="color: #83898d;">" .. "</span><span style="color: #83898d;">&gt;</span>
<span style="color: #83898d;">      </span><span style="color: #83898d;">&lt;!-- </span><span style="color: #83898d;">Config here </span><span style="color: #83898d;">--&gt;</span>
<span style="color: #83898d;">    &lt;/sqoop&gt;</span>

<span style="color: #83898d;">    &lt;ok to=</span><span style="color: #83898d;">"joining"</span><span style="color: #83898d;"> /&gt;</span>
<span style="color: #83898d;">    &lt;error to=</span><span style="color: #83898d;">"fail"</span><span style="color: #83898d;"> /&gt;</span>
<span style="color: #83898d;">  &lt;/action&gt;</span>

<span style="color: #83898d;">  &lt;join name=</span><span style="color: #83898d;">"joining"</span><span style="color: #83898d;"> to=</span><span style="color: #83898d;">"hive-node"</span><span style="color: #83898d;"> /&gt;</span>

<span style="color: #83898d;">  </span><span style="color: #83898d;">&lt;!-- </span><span style="color: #83898d;">Hive same as scoop with ok to="end" </span><span style="color: #83898d;">--&gt;</span>

<span style="color: #83898d;">  &lt;kill name=</span><span style="color: #83898d;">"fail"</span><span style="color: #83898d;">&gt;</span>
<span style="color: #83898d;">    &lt;message&gt; Message here &lt;/message&gt;</span>
<span style="color: #83898d;">  &lt;/kill&gt;</span>

<span style="color: #83898d;">  &lt;end name=</span><span style="color: #83898d;">"end"</span><span style="color: #83898d;"> /&gt;</span>
<span style="color: #83898d;">&lt;/workflow-app</span>&gt;

</pre>
</div>
</div>

<div id="outline-container-orgd08a914" class="outline-5">
<h5 id="orgd08a914">How to setup a workflow in Oozie</h5>
<div class="outline-text-5" id="text-orgd08a914">
<ul class="org-ul">
<li>Make sure each action works on its own - debugging in Oozie is a nightmare</li>
<li>Make a directory in HDFS for your job</li>
<li>Create workflow.xml and put in HDFS folder</li>
<li>Create job.properties to define any variables your workflow.xml needs
<ul class="org-ul">
<li>This goes in your local filesystem where you'll launch the job from</li>
<li>Can use these properties in your xml:</li>
</ul></li>
</ul>

<div class="org-src-container">
<pre class="src src-bash"><span style="color: #dcaeea;">nameNode</span>=hdfs://sandbox.hortonworks.com:8020
<span style="color: #dcaeea;">jobTask</span>=hdfs://sandbox.hortonworks.com:8050
<span style="color: #dcaeea;">queueName</span>=default
oozie.use.system.libpath=true
oozie.wf.application.path=${<span style="color: #dcaeea;">nameNode</span>}/user/maria_dev

</pre>
</div>


<p>
To run the workflow:
</p>

<div class="org-src-container">
<pre class="src src-bash">
oozie job --ozie http://localhost:1100/oozie -config /home/maria_dev/job.properties -run

</pre>
</div>

<p>
Monitor progress at <a href="http://127.0.0.1:1100/oozie">http://127.0.0.1:1100/oozie</a>
</p>
</div>
</div>
</div>

<div id="outline-container-orga993ff2" class="outline-4">
<h4 id="orga993ff2">Oozie Coordinator</h4>
<div class="outline-text-4" id="text-orga993ff2">
<ul class="org-ul">
<li>Schedules workflow execution.</li>
<li>XML &lt;coordinator-app &#x2026;&gt;</li>
<li>Runs the same as a workflow</li>
</ul>
</div>
</div>

<div id="outline-container-orge0f2f44" class="outline-4">
<h4 id="orge0f2f44">Oozie bundles</h4>
<div class="outline-text-4" id="text-orge0f2f44">
<ul class="org-ul">
<li>new in oozie 3.0</li>
<li>bundle is a collection of coordinators that can be managed together</li>
<li>Gives you control over the bundle (e.g. pause the whole bundle)</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org10100b4" class="outline-3">
<h3 id="org10100b4">Zeppelin</h3>
<div class="outline-text-3" id="text-org10100b4">
<p>
Notebook interface to your big data, lets you interactively run scripts / code against your data
</p>

<p>
go to localhost:9995 (default port), create new note to create notebook.
</p>
</div>
</div>

<div id="outline-container-org8a5aebb" class="outline-3">
<h3 id="org8a5aebb">Hue</h3>
<div class="outline-text-3" id="text-org8a5aebb">
<p>
Hadoop User Experience
(won't see it in hortonworks, mainly in cloudera)
</p>

<p>
Hortonworks uses ambari, cloudera uses Hue to show files UI and notebooks (not entirely ambari, but serves the same purpose)
</p>

<p>
Hue has a built-in graphical oozie editor and built-in notebooks.
</p>
</div>
</div>

<div id="outline-container-org0624114" class="outline-3">
<h3 id="org0624114">Other administrative technologies</h3>
<div class="outline-text-3" id="text-org0624114">
<p>
Some older techs in hadoop
</p>

<ul class="org-ul">
<li>Ganglia - monitoring system (ganglia is dead now), replaced by ambari etc</li>
<li>Chukwa - collecting and analyzing logs. Replaced by Flume and Kafka</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org5b7606f" class="outline-2">
<h2 id="org5b7606f">Feeding data into your cluster</h2>
<div class="outline-text-2" id="text-org5b7606f">
</div>
<div id="outline-container-org422348c" class="outline-3">
<h3 id="org422348c">Kafka</h3>
<div class="outline-text-3" id="text-org422348c">
<ul class="org-ul">
<li>pub/sub messaging system</li>
<li>Stores all incoming messages from publishers for some period of time, and publishes them to a stream of data called a topic</li>
<li>Consumers subscribe to one or more topics and receive data as it's published</li>
<li>Kafka can run in a cluster of its own to distribute processes on many servers (as well as storage of stream data)</li>
<li>Can distribute consumers too</li>
</ul>


<div class="figure">
<p><img src="./kafka.png" alt="kafka.png" width="400" />
</p>
<p><span class="figure-number">Figure 3: </span>Kafka cluster (from kafka.apache.org)</p>
</div>
</div>

<div id="outline-container-org6ab47ba" class="outline-4">
<h4 id="org6ab47ba">Using kafka</h4>
<div class="outline-text-4" id="text-org6ab47ba">
<ul class="org-ul">
<li>Can run kafka through ambari</li>
<li>Create a kafka topic (kafka uses zookeeper to keep track of which topics exists)</li>
</ul>

<div class="org-src-container">
<pre class="src src-bash">./kafka-topics.sh --create --zookeeper sandbox.hortonworks.com:2181 --replication-factor 1 --partitions 1 --topic fred
./kafka-topics.sh --list --zookeeper sandbox.hortonworks.com:2181
</pre>
</div>

<p>
<b>To publish a message</b>
</p>

<p>
This keeps a thread open where you can type messages that will be publishing messages
</p>

<div class="org-src-container">
<pre class="src src-bash">./kafka-console.producer.sh --broker-list sandbox.hortonworks.com:6667 --topic fred
Sending message
This is another a message
</pre>
</div>

<p>
<b>To consume a message</b>
</p>

<p>
This keeps a thread open where you can type messages that will be consuming messages
</p>

<p>
Ommitting &#x2013;from-beginning will only show new messages
</p>

<div class="org-src-container">
<pre class="src src-bash">./kafka-console.consumer.sh --bootstrap-server sandbox.hortonworks.com:6667 --zookeeper localhost:2181 --topic fred --from-beginning
</pre>
</div>


<div class="figure">
<p><img src="./kafka_pubsub.png" alt="kafka_pubsub.png" width="800" />
</p>
<p><span class="figure-number">Figure 4: </span>Kafka Produce / Consume</p>
</div>

<p>
<b>To use a connector</b>
</p>

<ul class="org-ul">
<li>To use a connector, create a copy of the config templates inside the conf folder in kafka files and modify</li>
<li>For a file connector:
<ul class="org-ul">
<li>Copy connect-standalone.properties and change the bootstrap.servers property to specify the server we're running on (sandbox.hortonworks.com:6667)</li>
<li>Copy connect-file-sink.properties and change 'file' (filename) which is where we'll write the results to and the topic that we're going to listen to ('topics')</li>
<li>Copy connect-file-source.properties and change 'file' (filename that we're listening to) and the topic we'll be writing to ('topic')</li>
<li>To run with properties, run: ./connect-standalone.sh standalone.properties source.properties sink.properties</li>
</ul></li>
</ul>
</div>
</div>
</div>


<div id="outline-container-orge205ebe" class="outline-3">
<h3 id="orge205ebe">Flume</h3>
<div class="outline-text-3" id="text-orge205ebe">
<p>
Made from the start with hadoop in mind. Has built-in sinks for HDFS and Hbase
</p>
</div>

<div id="outline-container-org77a5349" class="outline-4">
<h4 id="org77a5349">Anatomy of a flume agent and flow</h4>
<div class="outline-text-4" id="text-org77a5349">

<div class="figure">
<p><img src="./flume.png" alt="flume.png" width="600" />
</p>
</div>
</div>

<div id="outline-container-orgccb66c7" class="outline-5">
<h5 id="orgccb66c7">Source</h5>
<div class="outline-text-5" id="text-orgccb66c7">
<ul class="org-ul">
<li>Where the data is coming from</li>
<li>Can optionally have channel selectors and interceptors</li>
</ul>
</div>
</div>

<div id="outline-container-org866ee48" class="outline-5">
<h5 id="org866ee48">Channel</h5>
<div class="outline-text-5" id="text-org866ee48">
<ul class="org-ul">
<li>How the data is transferred (via memory or files)</li>
</ul>
</div>
</div>

<div id="outline-container-org43179eb" class="outline-5">
<h5 id="org43179eb">Sink</h5>
<div class="outline-text-5" id="text-org43179eb">
<ul class="org-ul">
<li>Where data is going</li>
<li>Can be organized into sing groups</li>
<li>Sink can connect to only one channel
<ul class="org-ul">
<li>Channel is notified to delete a message once the sink processes it</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb37771d" class="outline-2">
<h2 id="orgb37771d">Analyzing streams of data</h2>
<div class="outline-text-2" id="text-orgb37771d">
</div>
<div id="outline-container-org03f61de" class="outline-3">
<h3 id="org03f61de">Spark streaming</h3>
<div class="outline-text-3" id="text-org03f61de">
<ul class="org-ul">
<li>Spark cluster has receivers that receives streams of data</li>
<li>Not really real-time, it does it in micro-batches. But very close.</li>
<li>Abstraction on top of all of this is called a DStream ( Discretized Stream )</li>
<li>Apply processing on DStreams</li>
</ul>
</div>

<div id="outline-container-org21a928d" class="outline-4">
<h4 id="org21a928d">Windowed transformation</h4>
<div class="outline-text-4" id="text-org21a928d">
<ul class="org-ul">
<li>The batch interval is how often data is captured into a DStream</li>
<li>The slide interval is how often a windowed transformation is computed</li>
<li>The window interval is how far back in time the windowed transformation goes</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org5ea4183" class="outline-3">
<h3 id="org5ea4183">Apache Storm</h3>
<div class="outline-text-3" id="text-org5ea4183">
<p>
Storm works on individual events, not micro-batches
</p>

<p>
Different terms in storm:
</p>
<ul class="org-ul">
<li><i>Stream</i> consists of tuples that flow through</li>
<li><i>Spouts</i> that are sources of stream data (Kafka etc)</li>
<li><i>Bolts</i> that process stream data as it's received (ETL)</li>
<li>A <i>topology</i> is a graph of spouts and bolts that process your stream</li>
</ul>

<p>
Storm architecture:
</p>

<p>
Nimbus, pipes to multiple zookeepers pipes to multiple supervisors.
</p>

<p>
Developing storm is done in Java using either Storm Core or Trident
</p>
</div>
</div>

<div id="outline-container-org04c27bb" class="outline-3">
<h3 id="org04c27bb">Spark Streaming vs Storm</h3>
<div class="outline-text-3" id="text-org04c27bb">
<ul class="org-ul">
<li>Having the rest of spark is useful</li>
<li>Truly real-time processing = storm</li>
<li>Core storm offers "tumbling windows" in addition to "sliding windows"</li>
<li>Kafka + Storm seems to be a pretty good combination</li>
</ul>
</div>
</div>

<div id="outline-container-orgac9b146" class="outline-3">
<h3 id="orgac9b146">Apache Flink</h3>
<div class="outline-text-3" id="text-orgac9b146">
<p>
(As of the time of the video (2020), technology advancing fast
</p>

<ul class="org-ul">
<li>Highly scalable (1000s of nodes), similar to storm</li>
<li>Very strong at fault tolerance</li>
<li>faster than storm</li>
<li>Event-base stream processing like storm</li>
<li>has its own ecosystem like spark</li>
<li>youngest of techs between spark, storm and flink</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgd7c56f3" class="outline-2">
<h2 id="orgd7c56f3">Designing Real-world systems</h2>
<div class="outline-text-2" id="text-orgd7c56f3">
</div>
<div id="outline-container-orga973605" class="outline-3">
<h3 id="orga973605">Other good to mention techs</h3>
<div class="outline-text-3" id="text-orga973605">
</div>
<div id="outline-container-orgc88011b" class="outline-4">
<h4 id="orgc88011b">Impala</h4>
<div class="outline-text-4" id="text-orgc88011b">
<p>
Cloudera specific
</p>

<ul class="org-ul">
<li>Cloudera's alternative to Hive</li>
<li>Always running ready to run queries</li>
<li>Made for BI-style queries</li>
</ul>
</div>
</div>

<div id="outline-container-orgef4bbc6" class="outline-4">
<h4 id="orgef4bbc6">Accumulo</h4>
<div class="outline-text-4" id="text-orgef4bbc6">
<p>
Another BigTable clone like HBase
</p>
</div>
</div>

<div id="outline-container-orgc663bec" class="outline-4">
<h4 id="orgc663bec">Redis</h4>
<div class="outline-text-4" id="text-orgc663bec">
<p>
Distributed in-memory storage (like memcache)
</p>

<p>
Can also use it as persistent datastore
</p>
</div>
</div>

<div id="outline-container-org02d4b44" class="outline-4">
<h4 id="org02d4b44">Ignite</h4>
<div class="outline-text-4" id="text-org02d4b44">
<p>
Alternative to Redis
Closer to a database
</p>
<ul class="org-ul">
<li>ACID guarantee</li>
<li>SQL support</li>
<li>In memory processing</li>
</ul>
</div>
</div>

<div id="outline-container-org0e4643d" class="outline-4">
<h4 id="org0e4643d">Elasticsearch</h4>
<div class="outline-text-4" id="text-org0e4643d">
<p>
A distributed document search and analytics engine
</p>

<p>
Cool when combined with Kibana
</p>

<p>
Amazon has an ES service
</p>
</div>
</div>

<div id="outline-container-org2286c30" class="outline-4">
<h4 id="org2286c30">Kinesis (AWS)</h4>
<div class="outline-text-4" id="text-org2286c30">
<p>
AWS version of Kafka
</p>

<p>
(EMR is an easy way to spin up a hadoop cluster on demand)
</p>
</div>
</div>

<div id="outline-container-org39edaee" class="outline-4">
<h4 id="org39edaee">Apache Nifi</h4>
<div class="outline-text-4" id="text-org39edaee">
<p>
Directed graphs of data routing
</p>
</div>
</div>

<div id="outline-container-orgaea46f7" class="outline-4">
<h4 id="orgaea46f7">Falcon</h4>
<div class="outline-text-4" id="text-orgaea46f7">
<p>
A "data goverance engine" that sits on top of Oozie
</p>

<p>
Organize the flow of your data in hadoop
</p>
</div>
</div>

<div id="outline-container-org99e806e" class="outline-4">
<h4 id="org99e806e">Apache Slider</h4>
<div class="outline-text-4" id="text-org99e806e">
<p>
Deployment tool for apps on the YARN cluster
</p>
</div>
</div>
</div>

<div id="outline-container-org1dc3296" class="outline-3">
<h3 id="org1dc3296">Hadoop architecture design</h3>
<div class="outline-text-3" id="text-org1dc3296">
<p>
Some questions to ask and things to keep in mind
</p>

<ul class="org-ul">
<li>Working backwards:
<ul class="org-ul">
<li>Start with user's needs and then decide what to use</li>
</ul></li>
<li>What access patterns do you anticipate?
<ul class="org-ul">
<li>Analytics?</li>
<li>High transactions?</li>
</ul></li>
<li>Availability constraints?
<ul class="org-ul">
<li>availability &amp; consistency needs</li>
</ul></li>
<li>How big is the data? How often do you get data and how much?
<ul class="org-ul">
<li>Do you need a cluster?</li>
</ul></li>
<li>How much internal infrastructure and expertise is available?</li>
<li>Data retension?
<ul class="org-ul">
<li>Auditing?</li>
</ul></li>
<li>What about security?
<ul class="org-ul">
<li>Check with legal department</li>
</ul></li>
<li>Latency
<ul class="org-ul">
<li>How fast must users get results?</li>
</ul></li>
<li>Timelines
<ul class="org-ul">
<li>is batch ok?</li>
<li>does it need to be real-time?</li>
</ul></li>
<li>Does it make sense to re-use things already existing in your organisation?
<ul class="org-ul">
<li>Re-inventing the wheel is expensive and not always worth it</li>
</ul></li>
</ul>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: Mercia Malan</p>
<p class="date">Created: 2020-10-21 Wed 21:47</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
